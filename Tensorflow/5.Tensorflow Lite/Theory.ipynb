{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and components of mobile AI\n",
    "* **Lightweight**: This can lead to less accurate models. One example would be MobileNet.\n",
    "* **Low-latency**.\n",
    "* **Privacy**: There's no need to leave data from the device, improving privacy.\n",
    "* **Improved power consumption**: Normal models are very \"power hungry\".\n",
    "* **Efficient model format**.\n",
    "* **Pre-trained models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in TensorFlow lite\n",
    "1. **Converter**(to TensorFlow Lite format)\n",
    "    - Transforms TensorFlow models into a form efficient for reading the interpreter\n",
    "    - Introduces optimizations to improve binary size model performance and/or reduce model size\n",
    "    \n",
    "    \n",
    "2. **Interpreter**(Core)\n",
    "    - Diverse platform support (Android, iOS, embedded Linux and microcontrollers)\n",
    "    - Platform APIs for accelerated inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "After a training a model, it has to be converted before it can be used on a device.\n",
    "\n",
    "Steps to **convert** the TensorFlow model:\n",
    "\n",
    "1. Save the model in the recommended saved model format.\n",
    "2. Use the TensorFlow Lite converter tools to flatten the model to prepare it for mobile or embedded devices.\n",
    "\n",
    "Steps to use TF Lite:\n",
    "\n",
    "1. Jump start: Use pretrained or retrained models.\n",
    "2. Custom model: Develop and deploy custom model.\n",
    "3. Performance: Explore options, validate and accelerate models.\n",
    "4. Optimize: Model Optimization Toolkit.\n",
    "\n",
    "Inference in theses devices has to be performed very quicky in these devices because it's a very resource consuming task.For this purpose, TensorFlow Lite can employ hardware acceleration librarios or APIs for supported devices.\n",
    "\n",
    "Secondly, inference can be boosted with Edge TPUs as their solely built for operation on DL models. It's also used for training models.\n",
    "\n",
    "[GPU Delegate](https://www.youtube.com/watch?v=QSbAUxWfxQw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting and saving a model\n",
    "\n",
    "In python you can call `tf.lite.TFLiteConverter` to do the conversion. You can then instantiate your model from:\n",
    "\n",
    "1. SavedModel (preferred)\n",
    "\n",
    "2. Keras (a model instance HDF5 file)\n",
    "\n",
    "3. Concrete function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "\n",
    "pretrained_model = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# Saving the model for later use by tflite_convert\n",
    "pretrained_model.save('model.h5')\n",
    "\n",
    "# version_number = 1\n",
    "# export_dir = f\"/tmp/saved_model/{version_number}/\"\n",
    "\n",
    "# Export the SavedModel\n",
    "# tf.saved_model.save(pretrained_model, export_dir)\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(pretrained_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "tflite_model_file = pathlib.Path('/tmp/foo.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization techniques\n",
    "* **Quantization**: Reduces the precision of the numbers in the weights and biases of the model.\n",
    "    - All available CPU platforms are supported.\n",
    "    - Reduces latency and inference cost.\n",
    "    - It has low memory footprint.\n",
    "    - Allows execution on hardware restricted-to or optimized-for fixed-point operations.\n",
    "    - Optimized models for special purpose HW accelerators (TPUs).\n",
    "    \n",
    "    How it works?\n",
    "    - It converts all the floats in the weights of the model into ints.\n",
    "* **Weight pruning**: Reduces the overall number of parameters:\n",
    "* **Model topology transforms**: Allows you to get a more efficient model to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORFLOW LITE INTERPRETER IN PYTHON\n",
    "\n",
    "# Load TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Point the data to be used for testing and run the interpreter\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning on Cats vs Dogs\n",
    "\n",
    "1. **Prepare the dataset**: Download the data, split into sets (train, val, test), and preprocess.\n",
    "\n",
    "2. **Transfer Learning**: Choose a feature vector module (ex. MobileNet V2) from TFHub and perform transfer learning.\n",
    "\n",
    "3. **Export and convert**: Export the trained model to SavedModel and convert it to TFLite.\n",
    "\n",
    "4. **Deploy**: Deploy the converted model on a mobile device (Android/iOS/Linux/Microcontroller)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
